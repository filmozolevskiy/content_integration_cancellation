# Cursor Rules for Content Integration Cancellation Looker Project

## Project Overview
This is a Looker (LookML) project for content integration cancellation analytics. The project uses the "ota_phoenix" database connection (ClickHouse) and works with tables like `wenrix_rq_rs` and `booking_wenrix_cancellation_quote` containing JSON details.

## Code Style & Formatting

### LookML Files
- Use 2 spaces for indentation (no tabs)
- Always include proper labels for explores, views, and fields
- Use descriptive names following snake_case convention
- Add comments for complex logic or business rules
- Keep SQL queries readable with proper formatting

### SQL in Derived Tables
- Format SQL with proper indentation
- Use double semicolons (;;) to end SQL blocks
- Always test SQL queries independently before embedding in LookML
- Use CTEs for complex queries to improve readability
- Include comments for non-obvious SQL logic

## Project Structure

```
content_integration_cancellation/
├── models/
│   └── *.model.lkml          # LookML model files
├── views/
│   └── *.view.lkml           # LookML view files
└── docs/
    └── *.txt                 # Documentation files
```

- Models define explores and relationships
- Views contain dimensions, measures, and derived table logic
- Docs contain schema and data examples

## LookML Best Practices

### Views
- Always define a primary key using `sql_primary_key`
- Use descriptive labels for all dimensions and measures
- Include type definitions for dimensions (string, number, date, etc.)
- Add `hidden: yes` for internal fields that shouldn't be exposed
- Use `description` fields to document business logic

### Derived Tables
- Use `sql_table_name` for simple tables
- Use `derived_table` with SQL for computed tables
- Include `sql_trigger_value` for incremental PDTs when appropriate
- Always validate SQL syntax before committing

### Dimensions
- Use appropriate type: `string`, `number`, `date`, `yesno`, `tier`, `zipcode`, `location`
- **IMPORTANT**: Dimensions CANNOT be of type `datetime`. Only `dimension_group` can be of type `time`
- For datetime/timestamp fields, use `dimension_group` with `type: time` and specify `timeframes`
- Include `sql` for custom SQL dimensions
- Add `convert_tz` for proper timezone handling when needed

### Dimension Groups (Time Fields)
- **Always use `dimension_group` with `type: time` for datetime/timestamp fields**
- Specify appropriate `timeframes` array: `[raw, date, week, month, quarter, year, date_month_num, date_quarter_num, date_year]`
- Use `sql` to define the datetime expression
- Example:
  ```lookml
  dimension_group: created {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    sql: ${TABLE}.created_at ;;
  }
  ```

### Measures
- Prefer standard aggregations: `sum`, `count`, `average`, `min`, `max`
- Use `type: count` for counting records
- Use `type: sum` for summing numeric values
- Always include `sql_distinct_key` when using `distinct`
- Consider using `approximate_distinct_count` for large datasets

### Parameters
- Use descriptive parameter names
- Include default values when appropriate
- Use `allowed_value` for constrained options
- Add descriptions explaining parameter purpose

## Database & SQL Patterns

### Connection
- Database connection: `ota_phoenix` (ClickHouse)
- Primary tables: `ota_reports.wenrix_rq_rs`, `booking_wenrix_cancellation_quote`
- Database type: ClickHouse (distributed database)

### ClickHouse-Specific Functions

#### Hash Functions
- **DO NOT use `hash()` - it doesn't exist in ClickHouse**
- Use `cityHash64()` for hashing strings/data: `cityHash64(column_name)`
- Alternative hash functions: `xxHash64()`, `sipHash64()`, `MD5()`, `SHA256()`
- Example:
  ```sql
  toString(created_at) || '_' || toString(cityHash64(request)) || '_' || toString(cityHash64(response)) AS id
  ```

#### String Concatenation
- Use `||` operator: `string1 || string2`
- Or use `concat()` function: `concat(string1, string2, string3)`
- Use `toString()` to convert numbers/dates to strings: `toString(created_at)`

#### Date/Time Functions
- Use `parseDateTimeBestEffort()` for parsing datetime strings: `parseDateTimeBestEffort(timestamp_string)`
- Handle empty strings with `nullIf()` and `empty()`: `if(empty(nullIf(timestamp_string, '')), NULL, parseDateTimeBestEffort(...))`

### JSON Parsing Patterns (ClickHouse)
The JSON fields use ClickHouse JSON functions (different from MySQL):

```sql
-- Extract simple string fields
JSONExtractString(json_column, 'field_name') AS field_name

-- Extract nested fields
JSONExtractString(JSONExtractRaw(json_column, 'parent'), 'child') AS nested_field

-- Check if JSON field exists before extracting
if(JSONHas(json_column, 'field_name'), JSONExtractString(...), NULL) AS safe_field

-- Extract from arrays (first element at index 0)
JSONExtractString(JSONExtractRaw(json_column, 'errors', 0), 'code') AS error_code

-- Convert JSON values to numbers
toFloat64OrZero(JSONExtractString(...)) AS numeric_value
toInt32OrZero(JSONExtractString(...)) AS integer_value
```

### Common Patterns
- Always handle NULL values when extracting JSON - use `if(JSONHas(...), ...)` checks
- Use `JSONExtractRaw()` for nested objects, then `JSONExtractString()` for the final value
- Use `toFloat64OrZero()` or `toInt32OrZero()` for numeric conversions (handles NULL/empty safely)
- Test JSON extraction with sample data before implementing
- Use CTEs (Common Table Expressions) for complex JSON extraction to improve readability and performance

### SQL Best Practices
- Use table aliases (e.g., `b` for base_cte, `r` for request_cte)
- Use CTEs to organize complex queries and extract JSON fields once
- Filter on indexed columns when possible
- Use appropriate JOIN types (INNER, LEFT, etc.)
- Add WHERE clauses to filter data at the SQL level when possible
- Consider performance implications of derived tables - extract JSON in the derived table SQL, not in dimensions

## Error Handling

### LookML Validation
- Always validate LookML syntax before committing
- Check for required fields in explores
- Ensure proper SQL syntax in derived tables
- Verify field names match SQL aliases

### SQL Validation
- Test all SQL queries independently
- Handle NULL values appropriately
- Validate JSON extraction paths exist in sample data
- Consider error handling for malformed JSON

## Documentation

### Code Comments
- Add comments explaining business logic
- Document assumptions about data structure
- Note any data quality considerations
- Include examples of expected data formats

### Field Descriptions
- Always add `description` to complex dimensions/measures
- Explain calculation methodology
- Note any limitations or caveats
- Reference source documentation when applicable

## Testing & Development

### Development Workflow
1. Review documentation in `docs/` folder for schema details
2. Test SQL queries in database tool before implementing
3. Create views incrementally (dimensions first, then measures)
4. Validate LookML syntax after each change
5. Test explores in Looker UI

### Common Tasks
- When adding new dimensions, check JSON structure in docs
- When creating measures, consider appropriate aggregation
- When using dates, ensure timezone handling is correct
- When working with arrays, plan extraction strategy

## Performance Considerations

- Use persistent derived tables (PDTs) for expensive queries
- Set appropriate `datagroup_trigger` for PDTs
- Consider indexing strategy for filtered dimensions
- Use `sql_distinct_key` for distinct count measures
- Minimize JSON parsing in hot paths - consider preprocessing

## Naming Conventions

### Views
- Use descriptive names: `content_integration_cancellation`
- Match view name to business concept
- Use snake_case throughout

### Dimensions
- Use clear, business-friendly names
- Prefix date dimensions with time period if needed: `created_date`, `expires_date`
- Use descriptive suffixes: `_amount`, `_currency`, `_count`

### Measures
- Use aggregation prefix: `total_`, `average_`, `count_`, `max_`, `min_`
- Be specific: `total_refund_amount` not `total_amount`
- Use plural for counts: `cancellation_count`

### Explores
- Match explore name to primary view
- Use descriptive labels for end users
- Keep names consistent with view names

## Security & Access

- Review field visibility (`hidden: yes` for internal fields)
- Consider access filters if needed
- Document any PII handling requirements
- Follow data governance policies

## Git & Version Control

- Commit logical units of work
- Use descriptive commit messages
- Test before committing
- Keep derived table SQL readable for code review
- Document breaking changes

